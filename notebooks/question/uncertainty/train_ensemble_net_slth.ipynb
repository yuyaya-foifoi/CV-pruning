{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from src.data import get_data_loaders\n",
    "from src.models.resnet.resnet import ResNet18\n",
    "from src.pruning.slth.edgepopup import modify_module_for_slth\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_files_with_extension(base_path, extension):\n",
    "    files_with_extension = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                files_with_extension.append(os.path.join(root, file))\n",
    "    return files_with_extension\n",
    "\n",
    "def extract_number(file_path):\n",
    "    match = re.search(r'slth(\\d+)_state', file_path)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "def sort_files(file_list):\n",
    "    return sorted(\n",
    "        [file for file in file_list],\n",
    "        key=extract_number\n",
    "    )\n",
    "\n",
    "def filter_and_sort_files(file_list):\n",
    "    return sorted(\n",
    "        [file for file in file_list if extract_number(file) != 4],\n",
    "        key=extract_number\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleNet(nn.Module):\n",
    "    def __init__(self, num_models):\n",
    "        super(EnsembleNet, self).__init__()\n",
    "        # 畳み込み層\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # プーリング層\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 全結合層\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)  # 64チャネル、4x4画像に縮小\n",
    "        self.fc2 = nn.Linear(128, num_models)  # num_modelsは出力するモデルの数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 畳み込み + ReLU + プーリング\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # フラット化\n",
    "        x = x.view(x.size(0), -1)  # バッチサイズを保持しつつフラット化\n",
    "        # 全結合層 + ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 出力層\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"CIFAR10\"\n",
    "batch_size = 128\n",
    "device = 'cuda'\n",
    "\n",
    "# テストデータのロード\n",
    "train_loader, test_loader = get_data_loaders(dataset_name=dataset_name, batch_size=batch_size)\n",
    "\n",
    "# Base path template for the logs\n",
    "base_path_template = './logs/CIFAR10/is_prune/ensemble_output_diff_score/20240616_q2_1_remain_rate_010/seed_{}'\n",
    "extension = '.pkl'\n",
    "\n",
    "for seed in range(5):\n",
    "    base_path = base_path_template.format(seed)\n",
    "    file_list = get_files_with_extension(base_path, extension)\n",
    "    filtered_sorted_files = sort_files(file_list)\n",
    "\n",
    "    # モデルのリストを作成\n",
    "    models = []\n",
    "    for file in filtered_sorted_files:\n",
    "        resnet = ResNet18(10).to(device)\n",
    "        resnet = modify_module_for_slth(resnet, 0.10, is_print=False).to(device)\n",
    "        resnet.load_state_dict(torch.load(file))\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        resnet.eval()\n",
    "        models.append(resnet)\n",
    "\n",
    "    ensemble_net = EnsembleNet(len(models)).to(device)\n",
    "\n",
    "    # 損失関数とオプティマイザ\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ensemble_net.parameters(), lr=0.001)\n",
    "\n",
    "    # トレーニングループ\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ensemble_net.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 各モデルの出力を取得\n",
    "            with torch.no_grad():\n",
    "                outputs = [model(images) for model in models]\n",
    "            outputs = torch.stack(outputs, dim=-1)  # [batch_size, num_classes, num_models]\n",
    "\n",
    "            # 小型NNの出力をアンサンブル重みとして使用\n",
    "            optimizer.zero_grad()\n",
    "            ensemble_weights = ensemble_net(images)  # [batch_size, num_models]\n",
    "            ensemble_weights = nn.functional.softmax(ensemble_weights, dim=1)  # ソフトマックスで正規化\n",
    "\n",
    "            # アンサンブルの予測を計算\n",
    "            ensemble_output = torch.sum(outputs * ensemble_weights.unsqueeze(1), dim=-1)  # [batch_size, num_classes]\n",
    "\n",
    "            loss = criterion(ensemble_output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # テストデータでの評価\n",
    "    ensemble_net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 各モデルの出力を取得\n",
    "            outputs = [model(images) for model in models]\n",
    "            outputs = torch.stack(outputs, dim=-1)\n",
    "            \n",
    "            # 小型NNの出力をアンサンブル重みとして使用\n",
    "            ensemble_weights = ensemble_net(images)\n",
    "            ensemble_weights = nn.functional.softmax(ensemble_weights, dim=1)\n",
    "            \n",
    "            # アンサンブルの予測を計算\n",
    "            ensemble_output = torch.sum(outputs * ensemble_weights.unsqueeze(1), dim=-1)\n",
    "            \n",
    "            _, predicted = torch.max(ensemble_output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Ensemble Model Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def entropy(ensemble_outputs):\n",
    "    \"\"\"\n",
    "    アンサンブルの不確実性をエントロピーで計算する関数\n",
    "    \n",
    "    :param outputs_list: モデルの出力のリスト。各要素はshape (n_samples, n_classes) のtensor\n",
    "    :return: shape (n_samples,) の不確実性スコア\n",
    "    \"\"\"\n",
    "    # 各モデルの出力にsoftmaxを適用\n",
    "    #outputs_list = [F.softmax(output, dim=1) for output in outputs_list]\n",
    "    \n",
    "    # アンサンブルの平均予測を計算\n",
    "    #ensemble_outputs = torch.stack(outputs_list).mean(dim=0)\n",
    "    \n",
    "    # 数値の安定性のために、非常に小さい値をクリップ\n",
    "    ensemble_outputs = torch.clamp(ensemble_outputs, min=1e-8, max=1-1e-8)\n",
    "    \n",
    "    # エントロピーを計算\n",
    "    entropy = -torch.sum(ensemble_outputs * torch.log2(ensemble_outputs), dim=1)\n",
    "    \n",
    "    # nanをチェックし、必要に応じて0に置き換え\n",
    "    #entropy = torch.where(torch.isnan(entropy), torch.zeros_like(entropy), entropy)\n",
    "    #assert not torch.isnan(entropy), \"error\"\n",
    "    #print(entropy)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ネットワークとデータローダーの定義が必要です\n",
    "ensemble_net.eval()\n",
    "all_weights = []\n",
    "\n",
    "uncertainty_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = [model(images) for model in models]\n",
    "        outputs = torch.stack(outputs, dim=-1)\n",
    "        \n",
    "        ensemble_weights = ensemble_net(images)\n",
    "        ensemble_weights = nn.functional.softmax(ensemble_weights, dim=1)\n",
    "        ensemble_output = torch.sum(outputs * ensemble_weights.unsqueeze(1), dim=-1)\n",
    "        uncertainty = entropy(ensemble_output)\n",
    "        uncertainty_list.append(uncertainty.cpu().numpy())\n",
    "        \n",
    "        all_weights.append(ensemble_weights.cpu().numpy())  # CPUに移動してからnumpy配列に変換\n",
    "\n",
    "# 重みのリストをnumpy配列に変換\n",
    "all_weights = np.concatenate(all_weights, axis=0)\n",
    "\n",
    "# 平均値と標準偏差の計算\n",
    "mean_weights = np.mean(all_weights, axis=0)\n",
    "std_weights = np.std(all_weights, axis=0)\n",
    "\n",
    "# 棒グラフで可視化\n",
    "plt.bar(range(len(mean_weights)), mean_weights, yerr=std_weights, capsize=5)\n",
    "plt.xlabel('Model Index')\n",
    "plt.ylabel('Ensemble Weights')\n",
    "plt.title('Ensemble Weights Mean and Std Dev')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([np.mean(arr) for arr in uncertainty_list]))\n",
    "print(np.std([np.std(arr) for arr in uncertainty_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# seabornで棒グラフを作成\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(18, 12))\n",
    "bars = plt.bar(range(len(mean_weights)), mean_weights, yerr=std_weights, capsize=5, color='b')\n",
    "\n",
    "# meanの値を棒グラフの上に表示\n",
    "for bar, mean in zip(bars, mean_weights):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{mean:.4f}', ha='center', va='bottom', fontsize=32)\n",
    "\n",
    "plt.xlabel('Score Index', fontsize=32)\n",
    "plt.ylabel('Ensemble Weights', fontsize=32)\n",
    "plt.title('Ensemble Weights Mean and Std Dev', fontsize=32)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.yticks(fontsize=32)\n",
    "plt.ylim(0, 0.35)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
